% !TEX root=../proposal.tex

\section{Background}
\label{sec:background}

\subsection{Multi-Agent Reinforcement Learning Problem Setting}

In traditional single-agent reinforcement learning, the problem is set up as a
\emph{Markov Decision Process} (MDP). In a MDP problem setting, the agent can observe
a finite set of states $S$, take a finite set of actions $A$, has a state
transition function $T(s'|s,a) = P(S_{t+1}=s'|S_t=s, A_t=a)$, has a reward
function $r(s,a) = E[R_{t+1}|S_t=s, A_t=a]$, and a discount factor $\gamma$.
The state is fully observable to the agent, and the agent can decide a policy
$\pi(a|s) = P(A_t=a | S_t=s)$ for its next action based on the current state
$s$.

There are two ways for the agent to decide a good policy that maximizes the
return (i.e., discounted reward) $G_t = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$.
First, the agent can estimate the \emph{state-value function} of possible policies
$v_\pi(s) = E_\pi[G_t|S_t=s]$. Secondly, the agent can alternatively
estimate the \emph{action-value function} of possible policies
$q_\pi(s,a) = E_\pi[G_t|S_t=s, A_t=a]$. These functions essentially estimates
the expected return of a certain policy. To choose the best policy, the agent
estimates either of these value functions for a range of possible policies and
pick the one that has the highest value (i.e., expected return).

In multi-agent reinforcement learning, there are multiple agents, which leads
to several differences: (1)~each agent observes potentially different subsets
of states $O_1,\ldots,O_N$, which depend on the overall state, (2)~each agent
takes different actions $A_1,\ldots,A_N$, (3)~the state transition function
depends on the actions of all agents, (4)~each agent may have different goals
and thus have its own reward functions.


\subsection{Q-Learning and Deep Q-Learning}

Q-Learning and Deep Q-Learning (DQN) are popular techniques for single-agent
reinforcement learning, and can also be applied to solve multi-agent
reinforcement learning problems. Q-Learning uses a table to estimate the
optimal action-value function defined by Bellman equation. The table entries
are updated using stochastic gradient descent that minimizes the objective
function, i.e., the mean squared error (MSE) between the true action-value
function and the approximate Q-function.
\begin{equation}
J(w) = E_\pi[(q_\pi(S,A) - \hat q(S,A,w))^2]
\end{equation}
	
% The values for each table entry can be updated using
% dynamic programming, monte carlo methods, or temporal difference (TD) methods.
% \begin{align*}
% Q(S_t, A_t) \leftarrow Q(S_t, A_t) +
% 	\alpha [R_{t+1} + \gamma \max_\alpha Q(S_{t+1}, a) - Q(S_t, A_t)]
% \end{align*}

However, as the problem becomes complex (i.e., larger state and action
spaces), the table in traditional Q-Learning becomes too large to update
efficiently. Thus, DQN is developed to alleviate this problem by using a deep
neural network to approximate the optimal action-value function. The key idea
is to minimize the MSE between Q-network and Q-learning targets. The latter is
approximated by sampling from a replay memory $D$ which contains previous
state transitions. Each sample is used to update the Q-network parameters
using stochastic gradient descent to minimize the MSE objective function
mentioned above.
\begin{equation}
L(w) = E_{s,a,r,s'\sim D} [(r + \gamma \max_{a'} Q(s',a',w^-) - Q(s,a,w))^2]
\end{equation}

Both Q-Learning and DQN can be applied to a multi-agent reinforcement learning
by having each agent $i$ learn an independently optimal action-value function
$Q_i$. However, there are two fundamental problems of these techniques that
limit their capabilities. First, from any agent's point of view, since the
environment is affected by actions taken by other agents unknown to that
agent, the state transition function is no longer stationary, violating MDP
constraint necessary for Q-Learning. Second, for DQN, the experience replay
memory becomes inaccurate in approximating the state probabilities and state
transitions for a similar reason.


\subsection{Policy Gradient}

Policy Gradient (PG) is another popular technique for single-agent
reinforcement learning. Instead of approximating the state- or action-value
function and derive the optimal policy from it (like in DQN), the key idea of
PG is to directly parameterize the policy and learn the optimal policy
$\pi_\theta (s,a)$. There are several advantages of Policy Gradient over
value-based techniques: (1)~Policy Gradient often has better convergence
properties, (2)~Policy Gradient is effective in high-dimensional or continuous
action spaces, (3)~Policy Gradient can learn stochastic policies. The
objective of Policy Gradient is:
\begin{equation}
J(\theta) = E_{s\sim p^\pi, a\sim \pi} [\log \pi_\theta (a|s) Q^\pi(s,a)]
\end{equation}

Different policy gradient methods estimate $Q^\pi$ function differently, for
example, commonly used Monte-Carlo Policy Gradient (REINFORCE) simply
simulates the entire episode to obtain the overall return $G_t$. REINFORCE
updates the policy function parameters using stochastic gradient descent.
\begin{equation}
\theta \leftarrow \theta
	+ \alpha \gamma^t G_t \nabla_\theta \log \pi (A_t|S_t, \theta)
\end{equation}

Like many other single-agent techniques, PG can also be applied to a
multi-agent reinforcement learning problem. However, since the reward of each
agent can be highly dependent on other agents' actions the variance in the
policy gradient updates can be large, making it hard to converge to the
optimal policy and difficult to converge during training.


\subsection{Actor-Critic}

Actor-Critic mitigates the high variance problem of Policy Gradient techniques
by combining Policy Gradient with value function based techniques. Recall that
the objective function of Policy Gradient contains an estimation of the
action-value function $Q^w$. In REINFORCE, we simply use the overall return as
an approximation: $Q^w = G_t$, which is essentially sampling from the
distribution of action-values. In Actor-Critic method, we use a value-based
method, such as dynamic programming, monte-carlo method, temporal difference,
or DQN, to approximate $Q^w$, which reduces the variance of the policy
gradient updates.


\subsection{Evolutionary Algorithms}

Evolutionary algorithms are randomized algorithm which search through a
parameter space using operations such as random mutation and mixing of
parameter combinations~\cite{man1996genetic}. These sort of algorithms are
embarrassingly parallel and can be applied to optimize reinforcement learning
parameters. This sort of approach, which treats the reinforcement learning
problem as a black box, has been used to scale a Mujoco Humanoid problem to
1440 cores~\cite{salimans2017evolution}.

The idea of using these algorithms for RL is that the simplicity of brute
force trumps the complexity of other optimizers. Evolutionary algorithms have
low communication overhead, no backpropagation calculations, high parallelism
and an optimization algorithm free of stability issues, and they do this while
being competitive in training progress~\cite{salimans2017evolution}. Some of
the earliest RNNs were trained with random search, where they surpassed
backpropagation methods, so this trend has been seen before in the ML
community~\cite{hochreiter1997long}.

