% !TEX root=../proposal.tex

\section{Proposed Approach}
\label{sec:direction}

MADDPG is a recently developed general-purpose multi-agent learning algorithm
that is a simple extension of actor-critic policy gradient methods where the
critic is augmented with extra information about the policies of other agents
while the actor only has access to local information (i.e., its own
observations). In this framework of centralized training with decentralized
execution, agents don’t need to access the central critic at test time; they
learn approximate models of other agents and effectively use them in their own
policy learning procedure. Furthermore, since the centralized critic is
learned independently for each agent, this approach can in principle be used
to model arbitrary reward structures between agents, including adversarial
cases where the rewards are opposing.

The primary task we plan to apply MADDPG to is learning adaptive strategies
for coordinating/controlling traffic signals with the aim of increasing
traffic flow through multiple intersections. Unlike previous attempts that
employ Q learning to model adaptive traffic policies~\cite{araghi2015traffic},
we hypothesize that employing a centralized critic to supply agents (traffic
signals) with information about their peers’ observations (the amount of
traffic at intersections) and potential actions will lead to more stable
training and overcome the shortcomings of techniques not naturally suited to
multi-agent environments.

Furthermore, we will explore communication between multiple agents in the
presence of noisy channels. In the traffic signal coordination task, one can
easily imagine situations where communication channels between traffic signals
(assuming such channels existed in the first place) get corrupted or break. In
this scenario, agents will have to communicate reliably with one another
despite their messages being dropped or corrupted randomly. Prior work has
looked at using linear block error correcting codes designed by experts that
are then decoded by deep neural networks~\cite{nachmani2016learning,
nachmani2017rnn}. We plan to build on this work and incorporate it into the
MADDPG framework to increase the error tolerance.

Finally, we will investigate certain optimizations in training such as
evolutionary strategies, which have been shown to be competitive in training
to traditional methods while allowing increased
parallelism~\cite{salimans2017evolution}. If these methods prove successful,
they can be incorporated with other training methods to increase learning
scalability.

\input{sections/3_1_approach1.tex}
\input{sections/3_2_approach2.tex}
\input{sections/3_3_approach3.tex}